{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Frequently utilized function",
   "id": "b6336c3dd03237b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`plt.plot()`:\n",
    "\n",
    "`plt.ylim()\\ plt.xlim()`:\n",
    "\n",
    "`plt.xlabel()\\ plt.ylabel()`:\n",
    "\n",
    "`plt.legend([\"Class 0\", \"Class 1\"], loc=4)`:\n",
    "\n",
    "Adds a legend showing which marker/color represents each class. loc=4 means \"bottom-right\" of the plot.\n",
    "\n",
    "`(cancer.target_names, np.bincount(cancer.target))`"
   ],
   "id": "bb96c7271421c60f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# General",
   "id": "4aab5bd731988557"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Datasets that are included in scikit-learn are usually stored as\n",
    " ***Bunch*** objects, which contain some information about the dataset\n",
    " as well as the actual data. They behave like dictionaries, with the added benefit that you\n",
    " can access values using a dot (`bunch.key` instead of\n",
    " `bunch['key']`).\n",
    " \n",
    "By accessing the DESCR attribute of datasets, we can get more information."
   ],
   "id": "20b178cdf7417950"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Required Libraries",
   "id": "92dd5c6eeff97b22"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-24T06:40:29.077473Z",
     "start_time": "2025-03-24T06:40:29.036391Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "import mglearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_boston"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 16\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mneighbors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KNeighborsClassifier\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StandardScaler\n\u001B[1;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_boston\n",
      "File \u001B[1;32mE:\\python\\Lib\\site-packages\\sklearn\\datasets\\__init__.py:156\u001B[0m, in \u001B[0;36m__getattr__\u001B[1;34m(name)\u001B[0m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mload_boston\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    106\u001B[0m     msg \u001B[38;5;241m=\u001B[39m textwrap\u001B[38;5;241m.\u001B[39mdedent(\n\u001B[0;32m    107\u001B[0m \u001B[38;5;250m        \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    108\u001B[0m \u001B[38;5;124;03m        `load_boston` has been removed from scikit-learn since version 1.2.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;124;03m        \"\"\"\u001B[39;00m\n\u001B[0;32m    155\u001B[0m     )\n\u001B[1;32m--> 156\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(msg)\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    158\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mglobals\u001B[39m()[name]\n",
      "\u001B[1;31mImportError\u001B[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# A. (211-219)",
   "id": "59c5d5fd6003badc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T13:19:45.935429Z",
     "start_time": "2025-03-23T13:19:45.935429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The file has no headers naming the columns, so we pass header=None\n",
    "# and provide the column names explicitly in \"names\"\n",
    "data = pd.read_csv(\n",
    "\"/home/andy/datasets/adult.data\", header=None, index_col=False,\n",
    "names=['age', 'workclass', 'fnlwgt', 'education',  'education-num',\n",
    "'marital-status', 'occupation', 'relationship', 'race', 'gender',\n",
    "'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
    "'income'])\n",
    "# For illustration purposes, we only select some of the columns\n",
    "data = data[['age', 'workclass', 'education', 'gender', 'hours-per-week',\n",
    "'occupation', 'income']]\n",
    "# IPython.display allows nice output formatting within the Jupyter notebook\n",
    "display(data.head())\n",
    "print(data.gender.value_counts())\n",
    "print(\"Original features:\\n\", list(data.columns), \"\\n\") \n",
    "data_dummies = pd.get_dummies(data)\n",
    "print(\"Features after get_dummies:\\n\", list(data_dummies.columns))\n",
    "data_dummies.head()\n",
    "features = data_dummies.ix[:, 'age':'occupation_ Transport-moving']\n",
    "# Extract NumPy arrays\n",
    "X = features.values\n",
    "y = data_dummies['income_ >50K'].values\n",
    "print(\"X.shape: {}  y.shape: {}\".format(X.shape, y.shape))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "print(\"Test score: {:.2f}\".format(logreg.score(X_test, y_test)))\n",
    "# create a DataFrame with an integer feature and a categorical string feature\n",
    "demo_df = pd.DataFrame({'Integer Feature': [0, 1, 2, 1],\n",
    " 'Categorical Feature': ['socks', 'fox', 'socks', 'box']})\n",
    "display(demo_df)\n",
    "pd.get_dummies(demo_df)\n",
    "demo_df['Integer Feature'] = demo_df['Integer Feature'].astype(str)\n",
    "pd.get_dummies(demo_df, columns=['Integer Feature', 'Categorical Feature'])"
   ],
   "id": "147917edd661973d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# B. (236-241)",
   "id": "e13b5a3b21ee6146"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cancer = load_breast_cancer()\n",
    "# get deterministic random numbers\n",
    "rng = np.random.RandomState(42)\n",
    "noise = rng.normal(size=(len(cancer.data), 50))\n",
    "# add noise features to the data\n",
    "# the first 30 features are from the dataset, the next 50 are noise\n",
    "X_w_noise = np.hstack([cancer.data, noise])\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X_w_noise, cancer.target, random_state=0, test_size=.5)\n",
    "# use f_classif (the default) and SelectPercentile to select 50% of features\n",
    "select = SelectPercentile(percentile=50)\n",
    "select.fit(X_train, y_train)\n",
    "# transform training set\n",
    "X_train_selected = select.transform(X_train)\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))\n",
    "print(\"X_train_selected.shape: {}\".format(X_train_selected.shape))\n",
    "mask = select.get_support()\n",
    "print(mask)\n",
    "# visualize the mask -- black is True, white is False\n",
    "plt.matshow(mask.reshape(1, -1), cmap='gray_r')\n",
    "plt.xlabel(\"Sample index\")\n",
    "# transform test data\n",
    "X_test_selected = select.transform(X_test)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"Score with all features: {:.3f}\".format(lr.score(X_test,y_test)))\n",
    "lr.fit(X_train_selected, y_train)\n",
    "print(\"Score with only selected features: {:.3f}\".format(\n",
    "lr.score(X_test_selected, y_test)))\n",
    "select = SelectFromModel(\n",
    "RandomForestClassifier(n_estimators=100, random_state=42),\n",
    " threshold=\"median\")\n",
    "select.fit(X_train, y_train)\n",
    "X_train_l1 = select.transform(X_train)\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))\n",
    "print(\"X_train_l1.shape: {}\".format(X_train_l1.shape))\n",
    "mask = select.get_support()\n",
    "# visualize the mask -- black is True, white is False\n",
    "plt.matshow(mask.reshape(1, -1), cmap='gray_r')\n",
    "plt.xlabel(\"Sample index\")\n",
    "X_test_l1 = select.transform(X_test)\n",
    "score = LogisticRegression().fit(X_train_l1, y_train).score(X_test_l1, y_test)\n",
    "print(\"Test score: {:.3f}\".format(score))\n",
    "select = RFE(RandomForestClassifier(n_estimators=100, random_state=42),\n",
    " n_features_to_select=40)\n",
    "select.fit(X_train, y_train)\n",
    "# visualize the selected features:\n",
    "mask = select.get_support()\n",
    "plt.matshow(mask.reshape(1, -1), cmap='gray_r')\n",
    "plt.xlabel(\"Sample index\")\n",
    "X_train_rfe= select.transform(X_train)\n",
    "X_test_rfe= select.transform(X_test)\n",
    "score = LogisticRegression().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)\n",
    "print(\"Test score: {:.3f}\".format(score))\n",
    "print(\"Test score: {:.3f}\".format(select.score(X_test, y_test)))"
   ],
   "id": "be39994143ce2fa5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# C. (141-155)",
   "id": "52860f0f2872a36e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mglearn.plots.plot_pca_illustration()\n",
    "fig, axes = plt.subplots(15, 2, figsize=(10, 20))\n",
    "malignant = cancer.data[cancer.target == 0]\n",
    "benign = cancer.data[cancer.target == 1]\n",
    "ax = axes.ravel()\n",
    "for i in range(30):\n",
    " _, bins = np.histogram(cancer.data[:, i], bins=50)\n",
    " ax[i].hist(malignant[:, i], bins=bins, color=mglearn.cm3(0), alpha=.5)\n",
    " ax[i].hist(benign[:, i], bins=bins, color=mglearn.cm3(2), alpha=.5)\n",
    " ax[i].set_title(cancer.feature_names[i])\n",
    " ax[i].set_yticks(())\n",
    "ax[0].set_xlabel(\"Feature magnitude\")\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "ax[0].legend([\"malignant\", \"benign\"], loc=\"best\")\n",
    "fig.tight_layout()\n",
    "cancer = load_breast_cancer()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(cancer.data)\n",
    "X_scaled = scaler.transform(cancer.data)\n",
    "# keep the first two principal components of the data\n",
    "pca = PCA(n_components=2)\n",
    "# fit PCA model to breast cancer data\n",
    "pca.fit(X_scaled)\n",
    "# transform data onto the first two principal components\n",
    "X_pca = pca.transform(X_scaled)\n",
    "print(\"Original shape: {}\".format(str(X_scaled.shape)))\n",
    "print(\"Reduced shape: {}\".format(str(X_pca.shape)))\n",
    "# plot first vs. second principal component, colored by class\n",
    "plt.figure(figsize=(8, 8))\n",
    "mglearn.discrete_scatter(X_pca[:, 0], X_pca[:, 1], cancer.target)\n",
    "plt.legend(cancer.target_names, loc=\"best\")\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel(\"Second principal component\")\n",
    "print(\"PCA component shape: {}\".format(pca.components_.shape))\n",
    "print(\"PCA components:\\n{}\".format(pca.components_))\n",
    "plt.matshow(pca.components_, cmap='viridis')\n",
    "plt.yticks([0, 1], [\"First component\", \"Second component\"])\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(cancer.feature_names)),\n",
    "cancer.feature_names, rotation=60, ha='left')\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Principal components\")\n",
    "people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\n",
    "image_shape = people.images[0].shape\n",
    "fix, axes = plt.subplots(2, 5, figsize=(15, 8),\n",
    "subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for target, image, ax in zip(people.target, people.images, axes.ravel()):\n",
    " ax.imshow(image)\n",
    " ax.set_title(people.target_names[target])\n",
    " \n",
    "print(\"people.images.shape: {}\".format(people.images.shape))\n",
    "print(\"Number of classes: {}\".format(len(people.target_names)))\n",
    "# count how often each target appears\n",
    "counts = np.bincount(people.target)\n",
    "# print counts next to target names\n",
    "for i, (count, name) in enumerate(zip(counts, people.target_names)):\n",
    " print(\"{0:25} {1:3}\".format(name, count), end='   ')\n",
    " if (i + 1) % 3 == 0: print()\n",
    "mask = np.zeros(people.target.shape, dtype=np.bool)\n",
    "for target in np.unique(people.target):\n",
    " mask[np.where(people.target == target)[0][:50]] = 1\n",
    "X_people = people.data[mask]\n",
    "y_people = people.target[mask]\n",
    "# scale the grayscale values to be between 0 and 1\n",
    "# instead of 0 and 255 for better numeric stability\n",
    "X_people = X_people / 255.\n",
    "# split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X_people, y_people, stratify=y_people, random_state=0)\n",
    "# build a KNeighborsClassifier using one neighbor\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "print(\"Test set score of 1-nn: {:.2f}\".format(knn.score(X_test, y_test)))\n",
    "mglearn.plots.plot_pca_whitening()\n",
    "pca = PCA(n_components=100, whiten=True, random_state=0).fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print(\"X_train_pca.shape: {}\".format(X_train_pca.shape))\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train_pca, y_train)\n",
    "print(\"Test set accuracy: {:.2f}\".format(knn.score(X_test_pca, y_test)))\n",
    "print(\"pca.components_.shape: {}\".format(pca.components_.shape))\n",
    "fix, axes = plt.subplots(3, 5, figsize=(15, 12),\n",
    "subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for i, (component, ax) in enumerate(zip(pca.components_, axes.ravel())):\n",
    " ax.imshow(component.reshape(image_shape),\n",
    "cmap='viridis')\n",
    " ax.set_title(\"{}. component\".format((i + 1)))\n",
    "mglearn.plots.plot_pca_faces(X_train, X_test, image_shape)\n",
    "mglearn.discrete_scatter(X_train_pca[:, 0], X_train_pca[:, 1], y_train)\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel(\"Second principal component\")"
   ],
   "id": "d8f1dc29487b855c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# D. (30-39)",
   "id": "943442365621eb81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# generate dataset\n",
    "X, y = mglearn.datasets.make_forge()\n",
    "# plot dataset\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.legend([\"Class 0\", \"Class 1\"], loc=4)\n",
    "plt.xlabel(\"First feature\")\n",
    "plt.ylabel(\"Second feature\")\n",
    "print(\"X.shape: {}\".format(X.shape))\n",
    " \n",
    "X, y = mglearn.datasets.make_wave(n_samples=40)\n",
    "plt.plot(X, y, 'o')\n",
    "plt.ylim(-3, 3)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Target\")\n",
    " \n",
    "cancer = load_breast_cancer()\n",
    "print(\"cancer.keys(): \\n{}\".format(cancer.keys()))\n",
    " \n",
    "print(\"Shape of cancer data: {}\".format(cancer.data.shape))\n",
    "  \n",
    "print(\"Sample counts per class:\\n{}\".format(\n",
    "{n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))}))\n",
    "   \n",
    "print(\"Feature names:\\n{}\".format(cancer.feature_names))\n",
    "\n",
    "boston = load_boston()\n",
    "print(\"Data shape: {}\".format(boston.data.shape))\n",
    "\n",
    "X, y = mglearn.datasets.load_extended_boston()\n",
    "print(\"X.shape: {}\".format(X.shape))\n",
    "mglearn.plots.plot_knn_classification(n_neighbors=1)\n",
    "mglearn.plots.plot_knn_classification(n_neighbors=3)\n",
    "\n",
    "X, y = mglearn.datasets.make_forge()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Test set predictions: {}\".format(clf.predict(X_test)))\n",
    "print(\"Test set accuracy: {:.2f}\".format(clf.score(X_test, y_test)))\n",
    "   \n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "for n_neighbors, ax in zip([1, 3, 9], axes):\n",
    " # the fit method returns the object self, so we can instantiate\n",
    " # and fit in one line\n",
    " clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X, y)\n",
    " mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=.4)\n",
    " mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
    " ax.set_title(\"{} neighbor(s)\".format(n_neighbors))\n",
    " ax.set_xlabel(\"feature 0\")\n",
    " ax.set_ylabel(\"feature 1\")\n",
    "axes[0].legend(loc=3)   \n",
    " \n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "cancer.data, cancer.target, stratify=cancer.target, random_state=66)\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "# try n_neighbors from 1 to 10\n",
    "neighbors_settings = range(1, 11)\n",
    "for n_neighbors in neighbors_settings:\n",
    " # build the model\n",
    " clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    " clf.fit(X_train, y_train)\n",
    " # record training set accuracy\n",
    " training_accuracy.append(clf.score(X_train, y_train))\n",
    " # record generalization accuracy\n",
    " test_accuracy.append(clf.score(X_test, y_test))\n",
    " plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\n",
    "plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.legend()"
   ],
   "id": "990592152dfa5713",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`X, y = mglearn.datasets.make_forge()`\n",
    "\n",
    "It is a function from the `mglearn.datasets `module that:\n",
    "\n",
    "   - Creates a small, synthetic (fake) classification dataset.\n",
    "\n",
    "   - This dataset is designed specifically for learning and testing classifiers.\n",
    "\n",
    "   - It only has 26 samples and 2 features, so it’s easy to plot and visualize in 2D.\n",
    "\n",
    "X → a 2D NumPy array of shape (26, 2)\n",
    "- Each row is a sample (data point) & Each column is a feature (like \"height\" and \"weight\").\n",
    "\n",
    "y → a 1D NumPy array of shape (26,)\n",
    "- Contains labels (targets) for each sample. The values are 0 or 1, representing two classes\n",
    "\n",
    "`mglearn.discrete_scatter(X[:, 0], X[:, 1], y)`\n",
    "\n",
    "\n",
    "`X, y = mglearn.datasets.make_wave(n_samples=40)`\n",
    "\n",
    "Creates a new toy dataset — this one is for regression. It has a single input and a continuous target variable (response).\n",
    "   - 40 samples, 1 feature each\n",
    "   - y is continuous (not a class)\n",
    "\n",
    "We are using a low dimensional dataset because it's easier to visualize.\n",
    "*Any intuition derived from datasets with few features (low-dimensional datasets) might not hold in datasets with many features (high\n",
    "dimensional datasets)*\n",
    "\n",
    "`cancer = load_breast_cancer()`\n",
    "\n",
    "Loads a real dataset: breast cancer diagnosis\n",
    "\n",
    "   - 569 samples\n",
    "   - 30 features\n",
    "   - Target: 0 = malignant, 1 = benign\n",
    " \n",
    "`print(\"Shape of cancer data: {}\".format(cancer.data.shape))`:\n",
    "\n",
    "Prints shape of the data matrix.\n",
    "  \n",
    "`print(\"Sample counts per class:\\n{}\".format(\n",
    "{n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))}))`:\n",
    "\n",
    "Counts how many malignant and benign samples there are.\n",
    "   - np.bincount(cancer.target) counts how many 0s and 1s.\n",
    "   - zip matches the count to the name (malignant, benign)\n",
    "\n",
    "`boston = load_boston()`:\n",
    "\n",
    "A real-world regression dataset, the Boston Housing dataset. The task is to predict the median price of homes using information such as crime rate, proximity to the Charles River, highway accessibility, ect in several Boston neighborhoods in the 70s.\n",
    "The dataset contains 506 data points, each with 13 features.\n",
    "\n",
    "`X, y = mglearn.datasets.load_extended_boston()`\n",
    "\n",
    "Loads an extended version of the Boston dataset.Adds polynomial and interaction features → more complex. In this expanded version, products(interactions) of features are also considered (13 choose 2).\n",
    "- *Utilizing these derived features is called Feature engineering*\n",
    "\n",
    "`mglearn.plots.plot_knn_classification(n_neighbors=1)`:\n",
    "\n",
    "KNN is the simplest ML algorithm which consists only of *storing the training dataset*.\n",
    "\n",
    "\n",
    "`X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)`\n",
    "\n",
    "\n",
    "`clf = KNeighborsClassifier()`:\n",
    "\n",
    "\n",
    "`clf.fit(X_train, y_train)`:\n",
    "\n",
    "\n",
    "`print(\"Test set predictions: {}\".format(clf.predict(X_test)))`:\n",
    "\n",
    "\n",
    "`print(\"Test set accuracy: {:.2f}\".format(clf.score(X_test, y_test)))`:\n",
    "\n",
    "To evaluate how well our model generalizes, we use `.score()`.\n",
    "   \n",
    "`fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "for n_neighbors, ax in zip([1, 3, 9], axes)`:\n",
    "\n",
    "For n_neighbors = 1: Decision boundary is sharper, The model generalizes less and the model is the most complex.\n",
    "For n_neighbors = number of data points: Decision boundary is the smoothest, Generalizes the best, the model is the simplest.\n",
    "\n",
    "`clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X, y)`:\n",
    "\n",
    "\n",
    "`mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=.4)`:\n",
    "\n",
    "\n",
    "`mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)`:\n",
    "\n",
    "\n",
    "`axes[0].legend(loc=3)`:\n",
    "\n",
    "Adds a legend to the first subplot (bottom-left)   \n",
    " \n",
    "`cancer = load_breast_cancer()`\n",
    "`X_train, X_test, y_train, y_test = train_test_split(\n",
    "cancer.data, cancer.target, stratify=cancer.target, random_state=66)`\n",
    "`training_accuracy = []`\n",
    "`test_accuracy = []`\n",
    "\n",
    "`neighbors_settings = range(1, 11)`:\n",
    "`for n_neighbors in neighbors_settings:`:\n",
    "`clf = KNeighborsClassifier(n_neighbors=n_neighbors)`\n",
    " `clf.fit(X_train, y_train)`:\n",
    " `training_accuracy.append(clf.score(X_train, y_train))`:\n",
    " `test_accuracy.append(clf.score(X_test, y_test))`:\n",
    " \n",
    "- Real-world plots are rarely very smooth.\n",
    "- The test set accuracy for using a single neighbor is lower than when using more neighbors.\n",
    "- When considering neighbors as the number of data points, the model becomes too simple and the accuracy becomes worse.\n",
    "- The best performance is somewhere in the middle. Still, it is good to keep the scale of the plot in mind.\n"
   ],
   "id": "322bb20853a5d774"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
